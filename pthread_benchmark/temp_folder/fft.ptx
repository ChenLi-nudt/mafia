	.version 2.3
	.target sm_20
	.address_size 64
	// compiled with /i3c/hpcl/tzk133/software/cuda/open64/lib//be
	// nvopencc 4.0 built on 2011-05-12

	.visible .func _Z8GPU_FFT2R6float2S0_ (.param .u64 __cudaparmf1__Z8GPU_FFT2R6float2S0_, .param .u64 __cudaparmf2__Z8GPU_FFT2R6float2S0_)

	.visible .func _Z8GPU_FFT4R6float2S0_S0_S0_ (.param .u64 __cudaparmf1__Z8GPU_FFT4R6float2S0_S0_S0_, .param .u64 __cudaparmf2__Z8GPU_FFT4R6float2S0_S0_S0_, .param .u64 __cudaparmf3__Z8GPU_FFT4R6float2S0_S0_S0_, .param .u64 __cudaparmf4__Z8GPU_FFT4R6float2S0_S0_S0_)

	.visible .func (.param .s32 __cudaretf__Z10GPU_expandiii) _Z10GPU_expandiii (.param .s32 __cudaparmf1__Z10GPU_expandiii, .param .s32 __cudaparmf2__Z10GPU_expandiii, .param .s32 __cudaparmf3__Z10GPU_expandiii)

	.visible .func _Z16GPU_FftIterationiiP6float2S0_i (.param .s32 __cudaparmf1__Z16GPU_FftIterationiiP6float2S0_i, .param .s32 __cudaparmf2__Z16GPU_FftIterationiiP6float2S0_i, .param .u64 __cudaparmf3__Z16GPU_FftIterationiiP6float2S0_i, .param .u64 __cudaparmf4__Z16GPU_FftIterationiiP6float2S0_i, .param .s32 __cudaparmf5__Z16GPU_FftIterationiiP6float2S0_i)

	//-----------------------------------------------------------
	// Compiling fft.cpp3.i (/tmp/ccBI#.SPfZVr)
	//-----------------------------------------------------------

	//-----------------------------------------------------------
	// Options:
	//-----------------------------------------------------------
	//  Target:ptx, ISA:sm_20, Endian:little, Pointer Size:64
	//  -O3	(Optimization level)
	//  -g0	(Debug level)
	//  -m2	(Report advisories)
	//-----------------------------------------------------------

	.file	1	"<command-line>"
	.file	2	"fft.cudafe2.gpu"
	.file	3	"/usr/lib/gcc/x86_64-redhat-linux/4.4.7/include/stddef.h"
	.file	4	"/i3c/hpcl/tzk133/software/cuda/include/crt/device_runtime.h"
	.file	5	"/i3c/hpcl/tzk133/software/cuda/include/host_defines.h"
	.file	6	"/i3c/hpcl/tzk133/software/cuda/include/builtin_types.h"
	.file	7	"/i3c/hpcl/tzk133/software/cuda/include/device_types.h"
	.file	8	"/i3c/hpcl/tzk133/software/cuda/include/driver_types.h"
	.file	9	"/i3c/hpcl/tzk133/software/cuda/include/surface_types.h"
	.file	10	"/i3c/hpcl/tzk133/software/cuda/include/texture_types.h"
	.file	11	"/i3c/hpcl/tzk133/software/cuda/include/vector_types.h"
	.file	12	"/i3c/hpcl/tzk133/software/cuda/include/device_launch_parameters.h"
	.file	13	"/i3c/hpcl/tzk133/software/cuda/include/crt/storage_class.h"
	.file	14	"/usr/include/bits/types.h"
	.file	15	"/usr/include/time.h"
	.file	16	"FFT/fft.cu"
	.file	17	"/i3c/hpcl/tzk133/software/cuda/include/common_functions.h"
	.file	18	"/i3c/hpcl/tzk133/software/cuda/include/math_functions.h"
	.file	19	"/i3c/hpcl/tzk133/software/cuda/include/math_constants.h"
	.file	20	"/i3c/hpcl/tzk133/software/cuda/include/device_functions.h"
	.file	21	"/i3c/hpcl/tzk133/software/cuda/include/sm_11_atomic_functions.h"
	.file	22	"/i3c/hpcl/tzk133/software/cuda/include/sm_12_atomic_functions.h"
	.file	23	"/i3c/hpcl/tzk133/software/cuda/include/sm_13_double_functions.h"
	.file	24	"/i3c/hpcl/tzk133/software/cuda/include/sm_20_atomic_functions.h"
	.file	25	"/i3c/hpcl/tzk133/software/cuda/include/sm_20_intrinsics.h"
	.file	26	"/i3c/hpcl/tzk133/software/cuda/include/surface_functions.h"
	.file	27	"/i3c/hpcl/tzk133/software/cuda/include/texture_fetch_functions.h"
	.file	28	"/i3c/hpcl/tzk133/software/cuda/include/math_functions_dbl_ptx3.h"


	.visible .func _Z8GPU_FFT2R6float2S0_ (.param .u64 __cudaparmf1__Z8GPU_FFT2R6float2S0_, .param .u64 __cudaparmf2__Z8GPU_FFT2R6float2S0_)
	{
	.reg .u64 %rd<6>;
	.reg .f32 %f<12>;
	.loc	16	46	0
$LDWbegin__Z8GPU_FFT2R6float2S0_:
	ld.param.u64 	%rd1, [__cudaparmf1__Z8GPU_FFT2R6float2S0_];
	mov.s64 	%rd2, %rd1;
	ld.param.u64 	%rd3, [__cudaparmf2__Z8GPU_FFT2R6float2S0_];
	mov.s64 	%rd4, %rd3;
	ld.v2.f32 	{%f1,%f2}, [%rd2+0];
	ld.v2.f32 	{%f3,%f4}, [%rd4+0];
	.loc	16	48	0
	add.f32 	%f5, %f3, %f1;
	add.f32 	%f6, %f2, %f4;
	st.v2.f32 	[%rd2+0], {%f5,%f6};
	ld.v2.f32 	{%f7,%f8}, [%rd4+0];
	.loc	16	49	0
	sub.f32 	%f9, %f1, %f7;
	sub.f32 	%f10, %f2, %f8;
	st.v2.f32 	[%rd4+0], {%f9,%f10};
	.loc	16	50	0
	ret;
$LDWend__Z8GPU_FFT2R6float2S0_:
	} // _Z8GPU_FFT2R6float2S0_

	.visible .func _Z8GPU_FFT4R6float2S0_S0_S0_ (.param .u64 __cudaparmf1__Z8GPU_FFT4R6float2S0_S0_S0_, .param .u64 __cudaparmf2__Z8GPU_FFT4R6float2S0_S0_S0_, .param .u64 __cudaparmf3__Z8GPU_FFT4R6float2S0_S0_S0_, .param .u64 __cudaparmf4__Z8GPU_FFT4R6float2S0_S0_S0_)
	{
	.reg .u64 %rd<10>;
	.reg .f32 %f<43>;
	.loc	16	52	0
$LDWbegin__Z8GPU_FFT4R6float2S0_S0_S0_:
	ld.param.u64 	%rd1, [__cudaparmf1__Z8GPU_FFT4R6float2S0_S0_S0_];
	mov.s64 	%rd2, %rd1;
	ld.param.u64 	%rd3, [__cudaparmf2__Z8GPU_FFT4R6float2S0_S0_S0_];
	mov.s64 	%rd4, %rd3;
	ld.param.u64 	%rd5, [__cudaparmf3__Z8GPU_FFT4R6float2S0_S0_S0_];
	mov.s64 	%rd6, %rd5;
	ld.param.u64 	%rd7, [__cudaparmf4__Z8GPU_FFT4R6float2S0_S0_S0_];
	mov.s64 	%rd8, %rd7;
	ld.v2.f32 	{%f1,%f2}, [%rd2+0];
	ld.v2.f32 	{%f3,%f4}, [%rd6+0];
	.loc	16	48	0
	add.f32 	%f5, %f3, %f1;
	add.f32 	%f6, %f2, %f4;
	st.v2.f32 	[%rd2+0], {%f5,%f6};
	ld.v2.f32 	{%f7,%f8}, [%rd6+0];
	.loc	16	49	0
	sub.f32 	%f9, %f1, %f7;
	sub.f32 	%f10, %f2, %f8;
	st.v2.f32 	[%rd6+0], {%f9,%f10};
	ld.v2.f32 	{%f11,%f12}, [%rd4+0];
	ld.v2.f32 	{%f13,%f14}, [%rd8+0];
	.loc	16	48	0
	add.f32 	%f15, %f13, %f11;
	add.f32 	%f16, %f12, %f14;
	st.v2.f32 	[%rd4+0], {%f15,%f16};
	ld.v2.f32 	{%f17,%f18}, [%rd8+0];
	.loc	16	49	0
	sub.f32 	%f19, %f11, %f17;
	sub.f32 	%f20, %f12, %f18;
	st.v2.f32 	[%rd8+0], {%f19,%f20};
	.loc	16	55	0
	sub.f32 	%f21, %f17, %f11;
	st.v2.f32 	[%rd8+0], {%f20,%f21};
	ld.v2.f32 	{%f22,%f23}, [%rd2+0];
	ld.v2.f32 	{%f24,%f25}, [%rd4+0];
	.loc	16	48	0
	add.f32 	%f26, %f24, %f22;
	add.f32 	%f27, %f23, %f25;
	st.v2.f32 	[%rd2+0], {%f26,%f27};
	ld.v2.f32 	{%f28,%f29}, [%rd4+0];
	.loc	16	49	0
	sub.f32 	%f30, %f22, %f28;
	sub.f32 	%f31, %f23, %f29;
	st.v2.f32 	[%rd4+0], {%f30,%f31};
	ld.v2.f32 	{%f32,%f33}, [%rd6+0];
	ld.v2.f32 	{%f34,%f35}, [%rd8+0];
	.loc	16	48	0
	add.f32 	%f36, %f34, %f32;
	add.f32 	%f37, %f33, %f35;
	st.v2.f32 	[%rd6+0], {%f36,%f37};
	ld.v2.f32 	{%f38,%f39}, [%rd8+0];
	.loc	16	49	0
	sub.f32 	%f40, %f32, %f38;
	sub.f32 	%f41, %f33, %f39;
	st.v2.f32 	[%rd8+0], {%f40,%f41};
	.loc	16	58	0
	ret;
$LDWend__Z8GPU_FFT4R6float2S0_S0_S0_:
	} // _Z8GPU_FFT4R6float2S0_S0_S0_

	.visible .func (.param .s32 __cudaretf__Z10GPU_expandiii) _Z10GPU_expandiii (.param .s32 __cudaparmf1__Z10GPU_expandiii, .param .s32 __cudaparmf2__Z10GPU_expandiii, .param .s32 __cudaparmf3__Z10GPU_expandiii)
	{
	.reg .u32 %r<13>;
	.loc	16	108	0
$LDWbegin__Z10GPU_expandiii:
	ld.param.u32 	%r1, [__cudaparmf1__Z10GPU_expandiii];
	mov.s32 	%r2, %r1;
	ld.param.u32 	%r3, [__cudaparmf2__Z10GPU_expandiii];
	mov.s32 	%r4, %r3;
	ld.param.u32 	%r5, [__cudaparmf3__Z10GPU_expandiii];
	mov.s32 	%r6, %r5;
	.loc	16	109	0
	div.s32 	%r7, %r2, %r4;
	mul.lo.s32 	%r8, %r4, %r7;
	mul.lo.s32 	%r9, %r6, %r8;
	rem.s32 	%r10, %r2, %r4;
	add.s32 	%r11, %r9, %r10;
	st.param.s32 	[__cudaretf__Z10GPU_expandiii], %r11;
	ret;
$LDWend__Z10GPU_expandiii:
	} // _Z10GPU_expandiii
	.const .align 4 .b8 __cudart_i2opi_f[24] = {65,144,67,60,153,149,98,219,192,221,52,245,209,87,39,252,41,21,68,78,110,131,249,162};

	.visible .func _Z16GPU_FftIterationiiP6float2S0_i (.param .s32 __cudaparmf1__Z16GPU_FftIterationiiP6float2S0_i, .param .s32 __cudaparmf2__Z16GPU_FftIterationiiP6float2S0_i, .param .u64 __cudaparmf3__Z16GPU_FftIterationiiP6float2S0_i, .param .u64 __cudaparmf4__Z16GPU_FftIterationiiP6float2S0_i, .param .s32 __cudaparmf5__Z16GPU_FftIterationiiP6float2S0_i)
	{
	.reg .u32 %r<165>;
	.reg .u64 %rd<29>;
	.reg .f32 %f<88>;
	.reg .f64 %fd<7>;
	.reg .pred %p<23>;
	.local .align 8 .b8 __cuda___cuda_local_var_46130_10_non_const_v_320[16];
	.local .align 4 .b8 __cuda___cuda_result_4816[28];
	.local .align 4 .b8 __cuda___cuda_result_7644[28];
	.loc	16	112	0
$LDWbegin__Z16GPU_FftIterationiiP6float2S0_i:
	ld.param.u32 	%r1, [__cudaparmf1__Z16GPU_FftIterationiiP6float2S0_i];
	mov.s32 	%r2, %r1;
	ld.param.u32 	%r3, [__cudaparmf2__Z16GPU_FftIterationiiP6float2S0_i];
	mov.s32 	%r4, %r3;
	ld.param.u64 	%rd1, [__cudaparmf3__Z16GPU_FftIterationiiP6float2S0_i];
	mov.s64 	%rd2, %rd1;
	ld.param.u64 	%rd3, [__cudaparmf4__Z16GPU_FftIterationiiP6float2S0_i];
	mov.s64 	%rd4, %rd3;
	ld.param.u32 	%r5, [__cudaparmf5__Z16GPU_FftIterationiiP6float2S0_i];
	mov.s32 	%r6, %r5;
	.loc	16	115	0
	mov.s32 	%r7, 0;
	rem.s32 	%r8, %r2, %r4;
	cvta.local.u64 	%rd5, __cuda___cuda_local_var_46130_10_non_const_v_320;
	cvt.rn.f64.s32 	%fd1, %r8;
	mov.f64 	%fd2, 0dc01921fb54442d18;	// -6.28319
	mul.f64 	%fd3, %fd1, %fd2;
	mul.lo.s32 	%r9, %r4, 2;
	cvt.rn.f64.s32 	%fd4, %r9;
	div.rn.f64 	%fd5, %fd3, %fd4;
	cvt.rn.f32.f64 	%f1, %fd5;
	mov.s32 	%r10, 0;
	mov.u64 	%rd6, __cudart_i2opi_f;
	cvta.local.u64 	%rd7, __cuda___cuda_result_4816;
	cvta.local.u64 	%rd8, __cuda___cuda_result_7644;
$Lt_3_21506:
 //<loop> Loop body line 115, nesting depth: 1, iterations: 2
	.loc	16	118	0
	shr.s32 	%r11, %r7, 31;
	mov.s32 	%r12, 1;
	and.b32 	%r13, %r11, %r12;
	add.s32 	%r14, %r13, %r7;
	shr.s32 	%r15, %r14, 1;
	add.s32 	%r16, %r2, %r15;
	cvt.s64.s32 	%rd9, %r16;
	mul.wide.s32 	%rd10, %r16, 8;
	add.u64 	%rd11, %rd2, %rd10;
	ld.v2.f32 	{%f2,%f3}, [%rd11+0];
	st.v2.f32 	[%rd5+0], {%f2,%f3};
	.loc	16	119	0
	cvt.rn.f32.s32 	%f4, %r10;
	mul.f32 	%f5, %f4, %f1;
	mov.f32 	%f6, %f5;
	.loc	18	1858	0
	abs.f32 	%f7, %f5;
	mov.f32 	%f8, 0f7f800000;     	// ((1.0F)/(0.0F))
	setp.eq.f32 	%p1, %f7, %f8;
	@!%p1 bra 	$Lt_3_21762;
	.loc	18	1859	0
	mov.f32 	%f9, 0f00000000;     	// 0
	mul.rn.f32 	%f6, %f5, %f9;
$Lt_3_21762:
	.loc	18	1861	0
	abs.f32 	%f10, %f6;
	mov.f32 	%f11, 0f473ba700;    	// 48039
	setp.gt.f32 	%p2, %f10, %f11;
	@!%p2 bra 	$Lt_3_22274;
	.loc	18	1512	0
	mov.b32 	%r17, %f6;
	and.b32 	%r18, %r17, -2147483648;
	mov.s32 	%r19, %r18;
	.loc	18	24	0
	shl.b32 	%r20, %r17, 8;
	or.b32 	%r21, %r20, -2147483648;
	mov.s64 	%rd12, %rd6;
	cvta.local.u64 	%rd13, __cuda___cuda_result_4816;
	mov.s32 	%r22, 0;
	mov.u32 	%r23, 0;
$Lt_3_23298:
	.pragma "nounroll";
 //<loop> Loop body line 24, nesting depth: 2, iterations: 6
	.loc	18	1526	0
	ld.const.u32 	%r24, [%rd12+0];
	mul.lo.u32 	%r25, %r21, %r24;
	add.u32 	%r26, %r25, %r23;
	.loc	18	1527	0
	set.gt.u32.u32 	%r27, %r25, %r26;
	neg.s32 	%r28, %r27;
	mul.hi.u32 	%r29, %r24, %r21;
	add.u32 	%r23, %r28, %r29;
	.loc	18	1528	0
	st.u32 	[%rd13+0], %r26;
	add.s32 	%r22, %r22, 1;
	add.u64 	%rd13, %rd13, 4;
	add.u64 	%rd12, %rd12, 4;
	mov.u32 	%r30, 6;
	setp.ne.s32 	%p3, %r22, %r30;
	@%p3 bra 	$Lt_3_23298;
	.loc	18	1530	0
	st.local.u32 	[__cuda___cuda_result_4816+24], %r23;
	.loc	18	1535	0
	shl.b32 	%r31, %r17, 1;
	shr.u32 	%r32, %r31, 24;
	sub.u32 	%r33, %r32, 128;
	shr.u32 	%r34, %r33, 5;
	mov.s32 	%r35, 4;
	sub.s32 	%r36, %r35, %r34;
	cvt.s64.s32 	%rd14, %r36;
	mul.wide.s32 	%rd15, %r36, 4;
	add.u64 	%rd16, %rd7, %rd15;
	ld.u32 	%r23, [%rd16+8];
	.loc	18	1536	0
	ld.u32 	%r37, [%rd16+4];
	and.b32 	%r38, %r33, 31;
	mov.u32 	%r39, 0;
	setp.eq.u32 	%p4, %r38, %r39;
	@%p4 bra 	$Lt_3_23810;
	.loc	18	1539	0
	mov.s32 	%r40, 32;
	sub.s32 	%r41, %r40, %r38;
	shr.u32 	%r42, %r37, %r41;
	shl.b32 	%r43, %r23, %r38;
	add.u32 	%r23, %r42, %r43;
	.loc	18	1540	0
	ld.u32 	%r44, [%rd16+0];
	shr.u32 	%r45, %r44, %r41;
	shl.b32 	%r46, %r37, %r38;
	add.u32 	%r37, %r45, %r46;
$Lt_3_23810:
	.loc	18	1542	0
	shr.u32 	%r47, %r23, 30;
	.loc	18	1544	0
	shr.u32 	%r48, %r37, 30;
	shl.b32 	%r49, %r23, 2;
	add.u32 	%r23, %r48, %r49;
	.loc	18	1545	0
	shl.b32 	%r37, %r37, 2;
	.loc	18	1547	0
	shr.u32 	%r50, %r23, 31;
	add.u32 	%r51, %r47, %r50;
	.loc	18	1542	0
	neg.s32 	%r52, %r51;
	mov.u32 	%r53, 0;
	setp.ne.u32 	%p5, %r18, %r53;
	selp.s32 	%r22, %r52, %r51, %p5;
	mov.u32 	%r54, 0;
	setp.eq.u32 	%p6, %r50, %r54;
	@%p6 bra 	$Lt_3_24322;
	.loc	18	1552	0
	neg.s32 	%r37, %r37;
	.loc	18	1554	0
	mov.u32 	%r55, 0;
	set.eq.u32.u32 	%r56, %r37, %r55;
	neg.s32 	%r57, %r56;
	not.b32 	%r58, %r23;
	add.u32 	%r23, %r57, %r58;
	.loc	18	1555	0
	xor.b32 	%r19, %r18, -2147483648;
$Lt_3_24322:
	.loc	18	1557	0
	mov.s32 	%r59, %r22;
	.loc	18	1559	0
	clz.b32 	%r60, %r23;
	mov.s32 	%r61, %r60;
	.loc	18	1557	0
	mov.s32 	%r62, 32;
	sub.s32 	%r63, %r62, %r60;
	shr.u32 	%r64, %r37, %r63;
	shl.b32 	%r65, %r23, %r60;
	add.u32 	%r66, %r64, %r65;
	mov.u32 	%r67, 0;
	setp.ne.u32 	%p7, %r60, %r67;
	selp.u32 	%r68, %r66, %r23, %p7;
	.loc	18	1563	0
	mul.lo.u32 	%r37, %r68, -921707870;
	.loc	18	1564	0
	mov.u32 	%r69, -921707870;
	mul.hi.u32 	%r23, %r68, %r69;
	mov.u32 	%r70, 0;
	setp.le.s32 	%p8, %r23, %r70;
	@%p8 bra 	$Lt_3_24834;
	.loc	18	1566	0
	shr.u32 	%r71, %r37, 31;
	shl.b32 	%r72, %r23, 1;
	add.u32 	%r23, %r71, %r72;
	.loc	18	1567	0
	add.u32 	%r61, %r60, 1;
$Lt_3_24834:
	.loc	18	1570	0
	add.u32 	%r73, %r23, 1;
	shr.u32 	%r74, %r73, 7;
	add.u32 	%r75, %r74, 1;
	shr.u32 	%r76, %r75, 1;
	mov.s32 	%r77, 126;
	sub.s32 	%r78, %r77, %r61;
	shl.b32 	%r79, %r78, 23;
	add.u32 	%r80, %r76, %r79;
	or.b32 	%r81, %r19, %r80;
	mov.b32 	%f12, %r81;
	bra.uni 	$LDWendi___internal_trig_reduction_kernel_183_3;
$Lt_3_22274:
	.loc	18	1585	0
	mov.f32 	%f13, 0f3f22f983;    	// 0.63662
	mul.f32 	%f14, %f6, %f13;
	cvt.rni.s32.f32 	%r82, %f14;
	mov.s32 	%r59, %r82;
	.loc	18	1586	0
	cvt.rn.f32.s32 	%f15, %r82;
	neg.f32 	%f16, %f15;
	mov.f32 	%f17, 0f27c234c5;    	// 5.3903e-15
	mov.f32 	%f18, 0f33a22168;    	// 7.54979e-08
	mov.f32 	%f19, 0f3fc90fda;    	// 1.5708
	fma.rn.f32 	%f20, %f16, %f19, %f6;
	fma.rn.f32 	%f21, %f16, %f18, %f20;
	fma.rn.f32 	%f12, %f16, %f17, %f21;
$LDWendi___internal_trig_reduction_kernel_183_3:
	.loc	18	1861	0
	add.s32 	%r83, %r59, 1;
	mov.f32 	%f22, %f12;
	mul.f32 	%f23, %f22, %f22;
	and.b32 	%r84, %r83, 1;
	mov.u32 	%r85, 0;
	setp.eq.s32 	%p9, %r84, %r85;
	@%p9 bra 	$Lt_3_25602;
	.loc	18	1865	0
	mov.f32 	%f24, 0f37ccf5ce;    	// 2.44332e-05
	mov.f32 	%f25, 0fbab6061a;    	// -0.00138873
	fma.rn.f32 	%f26, %f24, %f23, %f25;
	mov.f32 	%f27, 0f3d2aaaa5;    	// 0.0416666
	fma.rn.f32 	%f28, %f26, %f23, %f27;
	mov.f32 	%f29, 0fbf000000;    	// -0.5
	fma.rn.f32 	%f30, %f28, %f23, %f29;
	mov.f32 	%f31, 0f3f800000;    	// 1
	fma.rn.f32 	%f32, %f30, %f23, %f31;
	bra.uni 	$Lt_3_25346;
$Lt_3_25602:
	.loc	18	1867	0
	mov.f32 	%f33, 0fb94ca1f9;    	// -0.000195153
	mov.f32 	%f34, 0f3c08839e;    	// 0.00833216
	fma.rn.f32 	%f35, %f33, %f23, %f34;
	mov.f32 	%f36, 0fbe2aaaa3;    	// -0.166667
	fma.rn.f32 	%f37, %f35, %f23, %f36;
	mul.f32 	%f38, %f23, %f37;
	fma.rn.f32 	%f32, %f38, %f22, %f22;
$Lt_3_25346:
	.loc	18	1869	0
	neg.f32 	%f39, %f32;
	and.b32 	%r86, %r83, 2;
	mov.s32 	%r87, 0;
	setp.ne.s32 	%p10, %r86, %r87;
	selp.f32 	%f32, %f39, %f32, %p10;
	.loc	18	1782	0
	mov.f32 	%f40, %f5;
	.loc	18	1752	0
	@!%p1 bra 	$Lt_3_25858;
	.loc	18	1753	0
	mov.f32 	%f41, 0f00000000;    	// 0
	mul.rn.f32 	%f40, %f5, %f41;
$Lt_3_25858:
	.loc	18	1755	0
	abs.f32 	%f42, %f40;
	mov.f32 	%f43, 0f473ba700;    	// 48039
	setp.gt.f32 	%p11, %f42, %f43;
	@!%p11 bra 	$Lt_3_26370;
	.loc	18	1512	0
	mov.b32 	%r88, %f40;
	and.b32 	%r89, %r88, -2147483648;
	mov.s32 	%r90, %r89;
	.loc	18	24	0
	shl.b32 	%r91, %r88, 8;
	or.b32 	%r92, %r91, -2147483648;
	mov.s64 	%rd17, %rd6;
	cvta.local.u64 	%rd18, __cuda___cuda_result_7644;
	mov.s32 	%r93, 0;
	mov.u32 	%r94, 0;
$Lt_3_27394:
	.pragma "nounroll";
 //<loop> Loop body line 24, nesting depth: 2, iterations: 6
	.loc	18	1526	0
	ld.const.u32 	%r95, [%rd17+0];
	mul.lo.u32 	%r96, %r92, %r95;
	add.u32 	%r97, %r96, %r94;
	.loc	18	1527	0
	set.gt.u32.u32 	%r98, %r96, %r97;
	neg.s32 	%r99, %r98;
	mul.hi.u32 	%r100, %r95, %r92;
	add.u32 	%r94, %r99, %r100;
	.loc	18	1528	0
	st.u32 	[%rd18+0], %r97;
	add.s32 	%r93, %r93, 1;
	add.u64 	%rd18, %rd18, 4;
	add.u64 	%rd17, %rd17, 4;
	mov.u32 	%r101, 6;
	setp.ne.s32 	%p12, %r93, %r101;
	@%p12 bra 	$Lt_3_27394;
	.loc	18	1530	0
	st.local.u32 	[__cuda___cuda_result_7644+24], %r94;
	.loc	18	1535	0
	shl.b32 	%r102, %r88, 1;
	shr.u32 	%r103, %r102, 24;
	sub.u32 	%r104, %r103, 128;
	shr.u32 	%r105, %r104, 5;
	mov.s32 	%r106, 4;
	sub.s32 	%r107, %r106, %r105;
	cvt.s64.s32 	%rd19, %r107;
	mul.wide.s32 	%rd20, %r107, 4;
	add.u64 	%rd21, %rd8, %rd20;
	ld.u32 	%r94, [%rd21+8];
	.loc	18	1536	0
	ld.u32 	%r108, [%rd21+4];
	and.b32 	%r109, %r104, 31;
	mov.u32 	%r110, 0;
	setp.eq.u32 	%p13, %r109, %r110;
	@%p13 bra 	$Lt_3_27906;
	.loc	18	1539	0
	mov.s32 	%r111, 32;
	sub.s32 	%r112, %r111, %r109;
	shr.u32 	%r113, %r108, %r112;
	shl.b32 	%r114, %r94, %r109;
	add.u32 	%r94, %r113, %r114;
	.loc	18	1540	0
	ld.u32 	%r115, [%rd21+0];
	shr.u32 	%r116, %r115, %r112;
	shl.b32 	%r117, %r108, %r109;
	add.u32 	%r108, %r116, %r117;
$Lt_3_27906:
	.loc	18	1542	0
	shr.u32 	%r118, %r94, 30;
	.loc	18	1544	0
	shr.u32 	%r119, %r108, 30;
	shl.b32 	%r120, %r94, 2;
	add.u32 	%r94, %r119, %r120;
	.loc	18	1545	0
	shl.b32 	%r108, %r108, 2;
	.loc	18	1547	0
	shr.u32 	%r121, %r94, 31;
	add.u32 	%r122, %r118, %r121;
	.loc	18	1542	0
	neg.s32 	%r123, %r122;
	mov.u32 	%r124, 0;
	setp.ne.u32 	%p14, %r89, %r124;
	selp.s32 	%r93, %r123, %r122, %p14;
	mov.u32 	%r125, 0;
	setp.eq.u32 	%p15, %r121, %r125;
	@%p15 bra 	$Lt_3_28418;
	.loc	18	1552	0
	neg.s32 	%r108, %r108;
	.loc	18	1554	0
	mov.u32 	%r126, 0;
	set.eq.u32.u32 	%r127, %r108, %r126;
	neg.s32 	%r128, %r127;
	not.b32 	%r129, %r94;
	add.u32 	%r94, %r128, %r129;
	.loc	18	1555	0
	xor.b32 	%r90, %r89, -2147483648;
$Lt_3_28418:
	.loc	18	1557	0
	mov.s32 	%r130, %r93;
	.loc	18	1559	0
	clz.b32 	%r131, %r94;
	mov.s32 	%r132, %r131;
	.loc	18	1557	0
	mov.s32 	%r133, 32;
	sub.s32 	%r134, %r133, %r131;
	shr.u32 	%r135, %r108, %r134;
	shl.b32 	%r136, %r94, %r131;
	add.u32 	%r137, %r135, %r136;
	mov.u32 	%r138, 0;
	setp.ne.u32 	%p16, %r131, %r138;
	selp.u32 	%r139, %r137, %r94, %p16;
	.loc	18	1563	0
	mul.lo.u32 	%r108, %r139, -921707870;
	.loc	18	1564	0
	mov.u32 	%r140, -921707870;
	mul.hi.u32 	%r94, %r139, %r140;
	mov.u32 	%r141, 0;
	setp.le.s32 	%p17, %r94, %r141;
	@%p17 bra 	$Lt_3_28930;
	.loc	18	1566	0
	shr.u32 	%r142, %r108, 31;
	shl.b32 	%r143, %r94, 1;
	add.u32 	%r94, %r142, %r143;
	.loc	18	1567	0
	add.u32 	%r132, %r131, 1;
$Lt_3_28930:
	.loc	18	1570	0
	add.u32 	%r144, %r94, 1;
	shr.u32 	%r145, %r144, 7;
	add.u32 	%r146, %r145, 1;
	shr.u32 	%r147, %r146, 1;
	mov.s32 	%r148, 126;
	sub.s32 	%r149, %r148, %r132;
	shl.b32 	%r150, %r149, 23;
	add.u32 	%r151, %r147, %r150;
	or.b32 	%r152, %r90, %r151;
	mov.b32 	%f44, %r152;
	bra.uni 	$LDWendi___internal_trig_reduction_kernel_183_1;
$Lt_3_26370:
	.loc	18	1585	0
	mov.f32 	%f45, 0f3f22f983;    	// 0.63662
	mul.f32 	%f46, %f40, %f45;
	cvt.rni.s32.f32 	%r153, %f46;
	mov.s32 	%r130, %r153;
	.loc	18	1586	0
	cvt.rn.f32.s32 	%f47, %r153;
	neg.f32 	%f48, %f47;
	mov.f32 	%f49, 0f27c234c5;    	// 5.3903e-15
	mov.f32 	%f50, 0f33a22168;    	// 7.54979e-08
	mov.f32 	%f51, 0f3fc90fda;    	// 1.5708
	fma.rn.f32 	%f52, %f48, %f51, %f40;
	fma.rn.f32 	%f53, %f48, %f50, %f52;
	fma.rn.f32 	%f44, %f48, %f49, %f53;
$LDWendi___internal_trig_reduction_kernel_183_1:
	.loc	18	1755	0
	mov.f32 	%f54, %f44;
	mul.f32 	%f55, %f54, %f54;
	and.b32 	%r154, %r130, 1;
	mov.u32 	%r155, 0;
	setp.eq.s32 	%p18, %r154, %r155;
	@%p18 bra 	$Lt_3_29698;
	.loc	18	1758	0
	mov.f32 	%f56, 0f37ccf5ce;    	// 2.44332e-05
	mov.f32 	%f57, 0fbab6061a;    	// -0.00138873
	fma.rn.f32 	%f58, %f56, %f55, %f57;
	mov.f32 	%f59, 0f3d2aaaa5;    	// 0.0416666
	fma.rn.f32 	%f60, %f58, %f55, %f59;
	mov.f32 	%f61, 0fbf000000;    	// -0.5
	fma.rn.f32 	%f62, %f60, %f55, %f61;
	mov.f32 	%f63, 0f3f800000;    	// 1
	fma.rn.f32 	%f64, %f62, %f55, %f63;
	bra.uni 	$Lt_3_29442;
$Lt_3_29698:
	.loc	18	1760	0
	mov.f32 	%f65, 0fb94ca1f9;    	// -0.000195153
	mov.f32 	%f66, 0f3c08839e;    	// 0.00833216
	fma.rn.f32 	%f67, %f65, %f55, %f66;
	mov.f32 	%f68, 0fbe2aaaa3;    	// -0.166667
	fma.rn.f32 	%f69, %f67, %f55, %f68;
	mul.f32 	%f70, %f55, %f69;
	fma.rn.f32 	%f64, %f70, %f54, %f54;
$Lt_3_29442:
	.loc	18	1762	0
	neg.f32 	%f71, %f64;
	and.b32 	%r156, %r130, 2;
	mov.s32 	%r157, 0;
	setp.ne.s32 	%p19, %r156, %r157;
	selp.f32 	%f64, %f71, %f64, %p19;
	mov.f32 	%f72, 0f00000000;    	// 0
	setp.eq.f32 	%p20, %f54, %f72;
	@!%p20 bra 	$Lt_3_29954;
	.loc	18	1766	0
	mov.f32 	%f73, 0f00000000;    	// 0
	mul.rn.f32 	%f64, %f54, %f73;
$Lt_3_29954:
	.loc	16	119	0
	mul.f32 	%f74, %f3, %f64;
	mul.f32 	%f75, %f2, %f32;
	sub.f32 	%f76, %f75, %f74;
	mul.f32 	%f77, %f3, %f32;
	fma.rn.f32 	%f78, %f2, %f64, %f77;
	st.v2.f32 	[%rd5+0], {%f76,%f78};
	add.s32 	%r10, %r10, 1;
	add.u64 	%rd5, %rd5, 8;
	add.s32 	%r7, %r7, %r6;
	mov.u32 	%r158, 2;
	setp.ne.s32 	%p21, %r10, %r158;
	@%p21 bra 	$Lt_3_21506;
	.loc	16	47	0
	ld.local.f32 	%f79, [__cuda___cuda_local_var_46130_10_non_const_v_320+0];
	ld.local.f32 	%f80, [__cuda___cuda_local_var_46130_10_non_const_v_320+4];
	.loc	16	48	0
	ld.local.f32 	%f81, [__cuda___cuda_local_var_46130_10_non_const_v_320+8];
	add.f32 	%f82, %f79, %f81;
	st.local.f32 	[__cuda___cuda_local_var_46130_10_non_const_v_320+0], %f82;
	ld.local.f32 	%f83, [__cuda___cuda_local_var_46130_10_non_const_v_320+12];
	add.f32 	%f84, %f80, %f83;
	st.local.f32 	[__cuda___cuda_local_var_46130_10_non_const_v_320+4], %f84;
	.loc	16	49	0
	sub.f32 	%f85, %f79, %f81;
	st.local.f32 	[__cuda___cuda_local_var_46130_10_non_const_v_320+8], %f85;
	sub.f32 	%f86, %f80, %f83;
	.loc	16	141	0
	div.s32 	%r159, %r2, %r4;
	mul.lo.s32 	%r160, %r159, %r4;
	mul.lo.s32 	%r161, %r160, 2;
	add.s32 	%r162, %r8, %r161;
	cvt.s64.s32 	%rd22, %r162;
	mul.wide.s32 	%rd23, %r162, 8;
	add.u64 	%rd24, %rd4, %rd23;
	st.v2.f32 	[%rd24+0], {%f82,%f84};
	add.s32 	%r163, %r162, %r4;
	cvt.s64.s32 	%rd25, %r163;
	mul.wide.s32 	%rd26, %r163, 8;
	add.u64 	%rd27, %rd4, %rd26;
	st.v2.f32 	[%rd27+0], {%f85,%f86};
	.loc	16	144	0
	ret;
$LDWend__Z16GPU_FftIterationiiP6float2S0_i:
	} // _Z16GPU_FftIterationiiP6float2S0_i

	.entry _Z14GPU_FFT_GlobaliP6float2S0_i (
		.param .s32 __cudaparm__Z14GPU_FFT_GlobaliP6float2S0_i_Ns,
		.param .u64 __cudaparm__Z14GPU_FFT_GlobaliP6float2S0_i___val_paramdata0,
		.param .u64 __cudaparm__Z14GPU_FFT_GlobaliP6float2S0_i___val_paramdata1,
		.param .s32 __cudaparm__Z14GPU_FFT_GlobaliP6float2S0_i_N)
	{
	.reg .u32 %r<164>;
	.reg .u64 %rd<31>;
	.reg .f32 %f<88>;
	.reg .f64 %fd<7>;
	.reg .pred %p<23>;
	.local .align 8 .b8 __cuda___cuda_local_var_46130_10_non_const_v_32104[16];
	.local .align 4 .b8 __cuda___cuda_result_48120[28];
	.local .align 4 .b8 __cuda___cuda_result_76148[28];
	.loc	16	146	0
$LDWbegin__Z14GPU_FFT_GlobaliP6float2S0_i:
	.loc	16	115	0
	ld.param.s32 	%r1, [__cudaparm__Z14GPU_FFT_GlobaliP6float2S0_i_N];
	mov.s32 	%r2, 0;
	ld.param.s32 	%r3, [__cudaparm__Z14GPU_FFT_GlobaliP6float2S0_i_Ns];
	cvt.s32.u32 	%r4, %tid.x;
	rem.s32 	%r5, %r4, %r3;
	mov.u32 	%r6, %ctaid.x;
	mul.lo.u32 	%r7, %r1, %r6;
	cvt.u64.u32 	%rd1, %r7;
	mul.wide.u32 	%rd2, %r7, 8;
	mov.u64 	%rd3, __cuda___cuda_local_var_46130_10_non_const_v_32104;
	ld.param.u64 	%rd4, [__cudaparm__Z14GPU_FFT_GlobaliP6float2S0_i___val_paramdata0];
	add.u64 	%rd5, %rd4, %rd2;
	cvt.rn.f64.s32 	%fd1, %r5;
	mov.f64 	%fd2, 0dc01921fb54442d18;	// -6.28319
	mul.f64 	%fd3, %fd1, %fd2;
	mul.lo.s32 	%r8, %r3, 2;
	cvt.rn.f64.s32 	%fd4, %r8;
	div.rn.f64 	%fd5, %fd3, %fd4;
	cvt.rn.f32.f64 	%f1, %fd5;
	mov.s32 	%r9, 0;
	mov.u64 	%rd6, __cuda___cuda_result_48120;
	mov.u64 	%rd7, __cudart_i2opi_f;
	mov.u64 	%rd8, __cuda___cuda_result_76148;
$Lt_4_21506:
 //<loop> Loop body line 115, nesting depth: 1, iterations: 2
	.loc	16	118	0
	shr.s32 	%r10, %r2, 31;
	mov.s32 	%r11, 1;
	and.b32 	%r12, %r10, %r11;
	add.s32 	%r13, %r12, %r2;
	shr.s32 	%r14, %r13, 1;
	add.s32 	%r15, %r4, %r14;
	cvt.s64.s32 	%rd9, %r15;
	mul.wide.s32 	%rd10, %r15, 8;
	add.u64 	%rd11, %rd5, %rd10;
	ld.global.v2.f32 	{%f2,%f3}, [%rd11+0];
	st.local.f32 	[%rd3+0], %f2;
	st.local.f32 	[%rd3+4], %f3;
	.loc	16	119	0
	cvt.rn.f32.s32 	%f4, %r9;
	mul.f32 	%f5, %f4, %f1;
	mov.f32 	%f6, %f5;
	.loc	18	1858	0
	abs.f32 	%f7, %f5;
	mov.f32 	%f8, 0f7f800000;     	// ((1.0F)/(0.0F))
	setp.eq.f32 	%p1, %f7, %f8;
	@!%p1 bra 	$Lt_4_21762;
	.loc	18	1859	0
	mov.f32 	%f9, 0f00000000;     	// 0
	mul.rn.f32 	%f6, %f5, %f9;
$Lt_4_21762:
	.loc	18	1861	0
	abs.f32 	%f10, %f6;
	mov.f32 	%f11, 0f473ba700;    	// 48039
	setp.gt.f32 	%p2, %f10, %f11;
	@!%p2 bra 	$Lt_4_22274;
	.loc	18	1512	0
	mov.b32 	%r16, %f6;
	and.b32 	%r17, %r16, -2147483648;
	mov.s32 	%r18, %r17;
	.loc	18	24	0
	shl.b32 	%r19, %r16, 8;
	or.b32 	%r20, %r19, -2147483648;
	mov.s64 	%rd12, %rd7;
	mov.u64 	%rd13, __cuda___cuda_result_48120;
	mov.s32 	%r21, 0;
	mov.u32 	%r22, 0;
$Lt_4_23298:
	.pragma "nounroll";
 //<loop> Loop body line 24, nesting depth: 2, iterations: 6
	.loc	18	1526	0
	ld.const.u32 	%r23, [%rd12+0];
	mul.lo.u32 	%r24, %r20, %r23;
	add.u32 	%r25, %r24, %r22;
	.loc	18	1527	0
	set.gt.u32.u32 	%r26, %r24, %r25;
	neg.s32 	%r27, %r26;
	mul.hi.u32 	%r28, %r23, %r20;
	add.u32 	%r22, %r27, %r28;
	.loc	18	1528	0
	st.local.u32 	[%rd13+0], %r25;
	add.s32 	%r21, %r21, 1;
	add.u64 	%rd13, %rd13, 4;
	add.u64 	%rd12, %rd12, 4;
	mov.u32 	%r29, 6;
	setp.ne.s32 	%p3, %r21, %r29;
	@%p3 bra 	$Lt_4_23298;
	.loc	18	1530	0
	st.local.u32 	[__cuda___cuda_result_48120+24], %r22;
	.loc	18	1535	0
	shl.b32 	%r30, %r16, 1;
	shr.u32 	%r31, %r30, 24;
	sub.u32 	%r32, %r31, 128;
	shr.u32 	%r33, %r32, 5;
	mov.s32 	%r34, 4;
	sub.s32 	%r35, %r34, %r33;
	cvt.s64.s32 	%rd14, %r35;
	mul.wide.s32 	%rd15, %r35, 4;
	add.u64 	%rd16, %rd6, %rd15;
	ld.local.u32 	%r22, [%rd16+8];
	.loc	18	1536	0
	ld.local.u32 	%r36, [%rd16+4];
	and.b32 	%r37, %r32, 31;
	mov.u32 	%r38, 0;
	setp.eq.u32 	%p4, %r37, %r38;
	@%p4 bra 	$Lt_4_23810;
	.loc	18	1539	0
	mov.s32 	%r39, 32;
	sub.s32 	%r40, %r39, %r37;
	shr.u32 	%r41, %r36, %r40;
	shl.b32 	%r42, %r22, %r37;
	add.u32 	%r22, %r41, %r42;
	.loc	18	1540	0
	ld.local.u32 	%r43, [%rd16+0];
	shr.u32 	%r44, %r43, %r40;
	shl.b32 	%r45, %r36, %r37;
	add.u32 	%r36, %r44, %r45;
$Lt_4_23810:
	.loc	18	1542	0
	shr.u32 	%r46, %r22, 30;
	.loc	18	1544	0
	shr.u32 	%r47, %r36, 30;
	shl.b32 	%r48, %r22, 2;
	add.u32 	%r22, %r47, %r48;
	.loc	18	1545	0
	shl.b32 	%r36, %r36, 2;
	.loc	18	1547	0
	shr.u32 	%r49, %r22, 31;
	add.u32 	%r50, %r46, %r49;
	.loc	18	1542	0
	neg.s32 	%r51, %r50;
	mov.u32 	%r52, 0;
	setp.ne.u32 	%p5, %r17, %r52;
	selp.s32 	%r21, %r51, %r50, %p5;
	mov.u32 	%r53, 0;
	setp.eq.u32 	%p6, %r49, %r53;
	@%p6 bra 	$Lt_4_24322;
	.loc	18	1552	0
	neg.s32 	%r36, %r36;
	.loc	18	1554	0
	mov.u32 	%r54, 0;
	set.eq.u32.u32 	%r55, %r36, %r54;
	neg.s32 	%r56, %r55;
	not.b32 	%r57, %r22;
	add.u32 	%r22, %r56, %r57;
	.loc	18	1555	0
	xor.b32 	%r18, %r17, -2147483648;
$Lt_4_24322:
	.loc	18	1557	0
	mov.s32 	%r58, %r21;
	.loc	18	1559	0
	clz.b32 	%r59, %r22;
	mov.s32 	%r60, %r59;
	.loc	18	1557	0
	mov.s32 	%r61, 32;
	sub.s32 	%r62, %r61, %r59;
	shr.u32 	%r63, %r36, %r62;
	shl.b32 	%r64, %r22, %r59;
	add.u32 	%r65, %r63, %r64;
	mov.u32 	%r66, 0;
	setp.ne.u32 	%p7, %r59, %r66;
	selp.u32 	%r67, %r65, %r22, %p7;
	.loc	18	1563	0
	mul.lo.u32 	%r36, %r67, -921707870;
	.loc	18	1564	0
	mov.u32 	%r68, -921707870;
	mul.hi.u32 	%r22, %r67, %r68;
	mov.u32 	%r69, 0;
	setp.le.s32 	%p8, %r22, %r69;
	@%p8 bra 	$Lt_4_24834;
	.loc	18	1566	0
	shr.u32 	%r70, %r36, 31;
	shl.b32 	%r71, %r22, 1;
	add.u32 	%r22, %r70, %r71;
	.loc	18	1567	0
	add.u32 	%r60, %r59, 1;
$Lt_4_24834:
	.loc	18	1570	0
	add.u32 	%r72, %r22, 1;
	shr.u32 	%r73, %r72, 7;
	add.u32 	%r74, %r73, 1;
	shr.u32 	%r75, %r74, 1;
	mov.s32 	%r76, 126;
	sub.s32 	%r77, %r76, %r60;
	shl.b32 	%r78, %r77, 23;
	add.u32 	%r79, %r75, %r78;
	or.b32 	%r80, %r18, %r79;
	mov.b32 	%f12, %r80;
	bra.uni 	$LDWendi___internal_trig_reduction_kernel_184_3;
$Lt_4_22274:
	.loc	18	1585	0
	mov.f32 	%f13, 0f3f22f983;    	// 0.63662
	mul.f32 	%f14, %f6, %f13;
	cvt.rni.s32.f32 	%r81, %f14;
	mov.s32 	%r58, %r81;
	.loc	18	1586	0
	cvt.rn.f32.s32 	%f15, %r81;
	neg.f32 	%f16, %f15;
	mov.f32 	%f17, 0f27c234c5;    	// 5.3903e-15
	mov.f32 	%f18, 0f33a22168;    	// 7.54979e-08
	mov.f32 	%f19, 0f3fc90fda;    	// 1.5708
	fma.rn.f32 	%f20, %f16, %f19, %f6;
	fma.rn.f32 	%f21, %f16, %f18, %f20;
	fma.rn.f32 	%f12, %f16, %f17, %f21;
$LDWendi___internal_trig_reduction_kernel_184_3:
	.loc	18	1861	0
	add.s32 	%r82, %r58, 1;
	mov.f32 	%f22, %f12;
	mul.f32 	%f23, %f22, %f22;
	and.b32 	%r83, %r82, 1;
	mov.u32 	%r84, 0;
	setp.eq.s32 	%p9, %r83, %r84;
	@%p9 bra 	$Lt_4_25602;
	.loc	18	1865	0
	mov.f32 	%f24, 0f37ccf5ce;    	// 2.44332e-05
	mov.f32 	%f25, 0fbab6061a;    	// -0.00138873
	fma.rn.f32 	%f26, %f24, %f23, %f25;
	mov.f32 	%f27, 0f3d2aaaa5;    	// 0.0416666
	fma.rn.f32 	%f28, %f26, %f23, %f27;
	mov.f32 	%f29, 0fbf000000;    	// -0.5
	fma.rn.f32 	%f30, %f28, %f23, %f29;
	mov.f32 	%f31, 0f3f800000;    	// 1
	fma.rn.f32 	%f32, %f30, %f23, %f31;
	bra.uni 	$Lt_4_25346;
$Lt_4_25602:
	.loc	18	1867	0
	mov.f32 	%f33, 0fb94ca1f9;    	// -0.000195153
	mov.f32 	%f34, 0f3c08839e;    	// 0.00833216
	fma.rn.f32 	%f35, %f33, %f23, %f34;
	mov.f32 	%f36, 0fbe2aaaa3;    	// -0.166667
	fma.rn.f32 	%f37, %f35, %f23, %f36;
	mul.f32 	%f38, %f23, %f37;
	fma.rn.f32 	%f32, %f38, %f22, %f22;
$Lt_4_25346:
	.loc	18	1869	0
	neg.f32 	%f39, %f32;
	and.b32 	%r85, %r82, 2;
	mov.s32 	%r86, 0;
	setp.ne.s32 	%p10, %r85, %r86;
	selp.f32 	%f32, %f39, %f32, %p10;
	.loc	18	1782	0
	mov.f32 	%f40, %f5;
	.loc	18	1752	0
	@!%p1 bra 	$Lt_4_25858;
	.loc	18	1753	0
	mov.f32 	%f41, 0f00000000;    	// 0
	mul.rn.f32 	%f40, %f5, %f41;
$Lt_4_25858:
	.loc	18	1755	0
	abs.f32 	%f42, %f40;
	mov.f32 	%f43, 0f473ba700;    	// 48039
	setp.gt.f32 	%p11, %f42, %f43;
	@!%p11 bra 	$Lt_4_26370;
	.loc	18	1512	0
	mov.b32 	%r87, %f40;
	and.b32 	%r88, %r87, -2147483648;
	mov.s32 	%r89, %r88;
	.loc	18	24	0
	shl.b32 	%r90, %r87, 8;
	or.b32 	%r91, %r90, -2147483648;
	mov.s64 	%rd17, %rd7;
	mov.u64 	%rd18, __cuda___cuda_result_76148;
	mov.s32 	%r92, 0;
	mov.u32 	%r93, 0;
$Lt_4_27394:
	.pragma "nounroll";
 //<loop> Loop body line 24, nesting depth: 2, iterations: 6
	.loc	18	1526	0
	ld.const.u32 	%r94, [%rd17+0];
	mul.lo.u32 	%r95, %r91, %r94;
	add.u32 	%r96, %r95, %r93;
	.loc	18	1527	0
	set.gt.u32.u32 	%r97, %r95, %r96;
	neg.s32 	%r98, %r97;
	mul.hi.u32 	%r99, %r94, %r91;
	add.u32 	%r93, %r98, %r99;
	.loc	18	1528	0
	st.local.u32 	[%rd18+0], %r96;
	add.s32 	%r92, %r92, 1;
	add.u64 	%rd18, %rd18, 4;
	add.u64 	%rd17, %rd17, 4;
	mov.u32 	%r100, 6;
	setp.ne.s32 	%p12, %r92, %r100;
	@%p12 bra 	$Lt_4_27394;
	.loc	18	1530	0
	st.local.u32 	[__cuda___cuda_result_76148+24], %r93;
	.loc	18	1535	0
	shl.b32 	%r101, %r87, 1;
	shr.u32 	%r102, %r101, 24;
	sub.u32 	%r103, %r102, 128;
	shr.u32 	%r104, %r103, 5;
	mov.s32 	%r105, 4;
	sub.s32 	%r106, %r105, %r104;
	cvt.s64.s32 	%rd19, %r106;
	mul.wide.s32 	%rd20, %r106, 4;
	add.u64 	%rd21, %rd8, %rd20;
	ld.local.u32 	%r93, [%rd21+8];
	.loc	18	1536	0
	ld.local.u32 	%r107, [%rd21+4];
	and.b32 	%r108, %r103, 31;
	mov.u32 	%r109, 0;
	setp.eq.u32 	%p13, %r108, %r109;
	@%p13 bra 	$Lt_4_27906;
	.loc	18	1539	0
	mov.s32 	%r110, 32;
	sub.s32 	%r111, %r110, %r108;
	shr.u32 	%r112, %r107, %r111;
	shl.b32 	%r113, %r93, %r108;
	add.u32 	%r93, %r112, %r113;
	.loc	18	1540	0
	ld.local.u32 	%r114, [%rd21+0];
	shr.u32 	%r115, %r114, %r111;
	shl.b32 	%r116, %r107, %r108;
	add.u32 	%r107, %r115, %r116;
$Lt_4_27906:
	.loc	18	1542	0
	shr.u32 	%r117, %r93, 30;
	.loc	18	1544	0
	shr.u32 	%r118, %r107, 30;
	shl.b32 	%r119, %r93, 2;
	add.u32 	%r93, %r118, %r119;
	.loc	18	1545	0
	shl.b32 	%r107, %r107, 2;
	.loc	18	1547	0
	shr.u32 	%r120, %r93, 31;
	add.u32 	%r121, %r117, %r120;
	.loc	18	1542	0
	neg.s32 	%r122, %r121;
	mov.u32 	%r123, 0;
	setp.ne.u32 	%p14, %r88, %r123;
	selp.s32 	%r92, %r122, %r121, %p14;
	mov.u32 	%r124, 0;
	setp.eq.u32 	%p15, %r120, %r124;
	@%p15 bra 	$Lt_4_28418;
	.loc	18	1552	0
	neg.s32 	%r107, %r107;
	.loc	18	1554	0
	mov.u32 	%r125, 0;
	set.eq.u32.u32 	%r126, %r107, %r125;
	neg.s32 	%r127, %r126;
	not.b32 	%r128, %r93;
	add.u32 	%r93, %r127, %r128;
	.loc	18	1555	0
	xor.b32 	%r89, %r88, -2147483648;
$Lt_4_28418:
	.loc	18	1557	0
	mov.s32 	%r129, %r92;
	.loc	18	1559	0
	clz.b32 	%r130, %r93;
	mov.s32 	%r131, %r130;
	.loc	18	1557	0
	mov.s32 	%r132, 32;
	sub.s32 	%r133, %r132, %r130;
	shr.u32 	%r134, %r107, %r133;
	shl.b32 	%r135, %r93, %r130;
	add.u32 	%r136, %r134, %r135;
	mov.u32 	%r137, 0;
	setp.ne.u32 	%p16, %r130, %r137;
	selp.u32 	%r138, %r136, %r93, %p16;
	.loc	18	1563	0
	mul.lo.u32 	%r107, %r138, -921707870;
	.loc	18	1564	0
	mov.u32 	%r139, -921707870;
	mul.hi.u32 	%r93, %r138, %r139;
	mov.u32 	%r140, 0;
	setp.le.s32 	%p17, %r93, %r140;
	@%p17 bra 	$Lt_4_28930;
	.loc	18	1566	0
	shr.u32 	%r141, %r107, 31;
	shl.b32 	%r142, %r93, 1;
	add.u32 	%r93, %r141, %r142;
	.loc	18	1567	0
	add.u32 	%r131, %r130, 1;
$Lt_4_28930:
	.loc	18	1570	0
	add.u32 	%r143, %r93, 1;
	shr.u32 	%r144, %r143, 7;
	add.u32 	%r145, %r144, 1;
	shr.u32 	%r146, %r145, 1;
	mov.s32 	%r147, 126;
	sub.s32 	%r148, %r147, %r131;
	shl.b32 	%r149, %r148, 23;
	add.u32 	%r150, %r146, %r149;
	or.b32 	%r151, %r89, %r150;
	mov.b32 	%f44, %r151;
	bra.uni 	$LDWendi___internal_trig_reduction_kernel_184_1;
$Lt_4_26370:
	.loc	18	1585	0
	mov.f32 	%f45, 0f3f22f983;    	// 0.63662
	mul.f32 	%f46, %f40, %f45;
	cvt.rni.s32.f32 	%r152, %f46;
	mov.s32 	%r129, %r152;
	.loc	18	1586	0
	cvt.rn.f32.s32 	%f47, %r152;
	neg.f32 	%f48, %f47;
	mov.f32 	%f49, 0f27c234c5;    	// 5.3903e-15
	mov.f32 	%f50, 0f33a22168;    	// 7.54979e-08
	mov.f32 	%f51, 0f3fc90fda;    	// 1.5708
	fma.rn.f32 	%f52, %f48, %f51, %f40;
	fma.rn.f32 	%f53, %f48, %f50, %f52;
	fma.rn.f32 	%f44, %f48, %f49, %f53;
$LDWendi___internal_trig_reduction_kernel_184_1:
	.loc	18	1755	0
	mov.f32 	%f54, %f44;
	mul.f32 	%f55, %f54, %f54;
	and.b32 	%r153, %r129, 1;
	mov.u32 	%r154, 0;
	setp.eq.s32 	%p18, %r153, %r154;
	@%p18 bra 	$Lt_4_29698;
	.loc	18	1758	0
	mov.f32 	%f56, 0f37ccf5ce;    	// 2.44332e-05
	mov.f32 	%f57, 0fbab6061a;    	// -0.00138873
	fma.rn.f32 	%f58, %f56, %f55, %f57;
	mov.f32 	%f59, 0f3d2aaaa5;    	// 0.0416666
	fma.rn.f32 	%f60, %f58, %f55, %f59;
	mov.f32 	%f61, 0fbf000000;    	// -0.5
	fma.rn.f32 	%f62, %f60, %f55, %f61;
	mov.f32 	%f63, 0f3f800000;    	// 1
	fma.rn.f32 	%f64, %f62, %f55, %f63;
	bra.uni 	$Lt_4_29442;
$Lt_4_29698:
	.loc	18	1760	0
	mov.f32 	%f65, 0fb94ca1f9;    	// -0.000195153
	mov.f32 	%f66, 0f3c08839e;    	// 0.00833216
	fma.rn.f32 	%f67, %f65, %f55, %f66;
	mov.f32 	%f68, 0fbe2aaaa3;    	// -0.166667
	fma.rn.f32 	%f69, %f67, %f55, %f68;
	mul.f32 	%f70, %f55, %f69;
	fma.rn.f32 	%f64, %f70, %f54, %f54;
$Lt_4_29442:
	.loc	18	1762	0
	neg.f32 	%f71, %f64;
	and.b32 	%r155, %r129, 2;
	mov.s32 	%r156, 0;
	setp.ne.s32 	%p19, %r155, %r156;
	selp.f32 	%f64, %f71, %f64, %p19;
	mov.f32 	%f72, 0f00000000;    	// 0
	setp.eq.f32 	%p20, %f54, %f72;
	@!%p20 bra 	$Lt_4_29954;
	.loc	18	1766	0
	mov.f32 	%f73, 0f00000000;    	// 0
	mul.rn.f32 	%f64, %f54, %f73;
$Lt_4_29954:
	.loc	16	119	0
	mul.f32 	%f74, %f3, %f64;
	mul.f32 	%f75, %f2, %f32;
	sub.f32 	%f76, %f75, %f74;
	st.local.f32 	[%rd3+0], %f76;
	mul.f32 	%f77, %f3, %f32;
	fma.rn.f32 	%f78, %f2, %f64, %f77;
	st.local.f32 	[%rd3+4], %f78;
	add.s32 	%r9, %r9, 1;
	add.u64 	%rd3, %rd3, 8;
	add.s32 	%r2, %r2, %r1;
	mov.u32 	%r157, 2;
	setp.ne.s32 	%p21, %r9, %r157;
	@%p21 bra 	$Lt_4_21506;
	.loc	16	47	0
	ld.local.f32 	%f79, [__cuda___cuda_local_var_46130_10_non_const_v_32104+0];
	ld.local.f32 	%f80, [__cuda___cuda_local_var_46130_10_non_const_v_32104+4];
	.loc	16	48	0
	ld.local.f32 	%f81, [__cuda___cuda_local_var_46130_10_non_const_v_32104+8];
	add.f32 	%f82, %f79, %f81;
	st.local.f32 	[__cuda___cuda_local_var_46130_10_non_const_v_32104+0], %f82;
	ld.local.f32 	%f83, [__cuda___cuda_local_var_46130_10_non_const_v_32104+12];
	add.f32 	%f84, %f80, %f83;
	st.local.f32 	[__cuda___cuda_local_var_46130_10_non_const_v_32104+4], %f84;
	.loc	16	49	0
	sub.f32 	%f85, %f79, %f81;
	st.local.f32 	[__cuda___cuda_local_var_46130_10_non_const_v_32104+8], %f85;
	sub.f32 	%f86, %f80, %f83;
	.loc	16	141	0
	div.s32 	%r158, %r4, %r3;
	mul.lo.s32 	%r159, %r158, %r3;
	mul.lo.s32 	%r160, %r159, 2;
	ld.param.u64 	%rd22, [__cudaparm__Z14GPU_FFT_GlobaliP6float2S0_i___val_paramdata1];
	add.u64 	%rd23, %rd22, %rd2;
	add.s32 	%r161, %r5, %r160;
	cvt.s64.s32 	%rd24, %r161;
	mul.wide.s32 	%rd25, %r161, 8;
	add.u64 	%rd26, %rd23, %rd25;
	st.global.v2.f32 	[%rd26+0], {%f82,%f84};
	add.s32 	%r162, %r161, %r3;
	cvt.s64.s32 	%rd27, %r162;
	mul.wide.s32 	%rd28, %r162, 8;
	add.u64 	%rd29, %rd23, %rd28;
	st.global.v2.f32 	[%rd29+0], {%f85,%f86};
	.loc	16	150	0
	exit;
$LDWend__Z14GPU_FFT_GlobaliP6float2S0_i:
	} // _Z14GPU_FFT_GlobaliP6float2S0_i

